{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "-q9lkPZ64-BU",
    "outputId": "1e85d23c-1be2-4f0c-9f8f-11e80c96e975"
   },
   "outputs": [],
   "source": [
    "! pip install hazm\n",
    "from __future__ import unicode_literals\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from hazm import Normalizer, Stemmer, word_tokenize, sent_tokenize\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, RNN, GRU, LSTM, Lambda, concatenate, Dense, Embedding, Dropout, Activation, Conv1D, MaxPooling1D, Flatten, Multiply, GlobalMaxPooling1D, TimeDistributed, SpatialDropout1D, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "0mOaRhb25tgD",
    "outputId": "4197c4d9-f5b5-41c6-9d11-48dfad324e43"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NG-NRWVR4-Be"
   },
   "outputs": [],
   "source": [
    "class data_utils:\n",
    "    \n",
    "    def __init__(self, texts, labels):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_vec = {}\n",
    "        self.vocab_dict = {}\n",
    "        self.vec_dim = 0\n",
    "        self.label_dict = {}\n",
    "        count = 0\n",
    "        for l in labels:\n",
    "            if l not in self.label_dict:\n",
    "                self.label_dict[l] = count\n",
    "                count += 1\n",
    "                \n",
    "    def create_embeddings(self, dimension=100):\n",
    "        \n",
    "        self.vec_dim = dimension\n",
    "        data = []\n",
    "        for d in self.texts:\n",
    "            for i in sent_tokenize(d): \n",
    "                temp = [] \n",
    "                for j in word_tokenize(i): \n",
    "                    temp.append(j) \n",
    "\n",
    "                data.append(temp)\n",
    "\n",
    "        model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                                      size = dimension, window = 5)\n",
    "        model1.train(data, total_examples=len(data), epochs=30)\n",
    "        self.word_vec = {}\n",
    "        for w in model1.wv.vocab:\n",
    "            self.word_vec[w] = model1.wv.get_vector(w)\n",
    "    \n",
    "            \n",
    "    def create_vocab(self, vocab_size):\n",
    "        \n",
    "        train_tok_sents = list(map(word_tokenize, self.texts))\n",
    "        special_toks = ['<pad>','<unk>']\n",
    "        c = Counter()\n",
    "        for s in train_tok_sents:\n",
    "            c.update(s)\n",
    "\n",
    "        words = [w for w,_ in c.most_common(vocab_size)] \n",
    "        vocab = special_toks + words\n",
    "        self.vocab_dict = {w:i for i, w in enumerate(vocab)}\n",
    "    \n",
    "    def create_embedding_matrix(self):\n",
    "        \n",
    "        mean_embed = np.mean(np.array(list(self.word_vec.values())), axis=0)\n",
    "        embedding_matrix = np.zeros((len(self.vocab_dict), self.vec_dim))\n",
    "        for word, i in self.vocab_dict.items():\n",
    "            embedding_matrix[i] = self.word_vec.get(word, mean_embed)\n",
    "            \n",
    "        embedding_matrix[0] = np.zeros(shape=(self.vec_dim, ))\n",
    "        return embedding_matrix\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bq0bQltj5Q1U"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_loss_and_acc(history):\n",
    "  history_dict = history.history\n",
    "  loss_values = history_dict['loss']\n",
    "  val_loss_values = history_dict['val_loss']\n",
    "  acc = history_dict['acc']\n",
    "\n",
    "  epochs = range(1, len(acc) + 1)\n",
    "\n",
    "  f = plt.figure(figsize=(10,3))\n",
    "\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  acc_values = history_dict['acc']\n",
    "  val_acc = history_dict['val_acc']\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUBU0Twa4-Bh"
   },
   "outputs": [],
   "source": [
    "# zip_ref = zipfile.ZipFile('/content/drive/My Drive/file.zip', 'r')\n",
    "zip_ref = zipfile.ZipFile('./file.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQss0Tjr-bNg"
   },
   "outputs": [],
   "source": [
    "texts, labels = pickle.load(open('./file', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TT3_gA1L4-Bj"
   },
   "outputs": [],
   "source": [
    "train_sents, test_sents, train_labels, test_labels = train_test_split(texts, labels, stratify=labels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQlGN2lH4-Bm"
   },
   "outputs": [],
   "source": [
    "du = data_utils(train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PQA07nrx4-Bo",
    "outputId": "d844e5b5-3cb6-49e5-a276-e5b7c176631d"
   },
   "outputs": [],
   "source": [
    "du.create_embeddings(dimension=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gbbK9Rv4-Br"
   },
   "outputs": [],
   "source": [
    "du.create_vocab(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0c8Cf3M4-Bw"
   },
   "source": [
    "# SVM with BOW+tfi-df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRG6ey694-Bx"
   },
   "outputs": [],
   "source": [
    "NGRAM_RANGE = 3\n",
    "for feature in ['tfidf', True, False]:\n",
    "    \n",
    "    vec = TfidfVectorizer(ngram_range=(1,NGRAM_RANGE)) if feature=='tfidf' else CountVectorizer(binary=feature, ngram_range=(1,NGRAM_RANGE))\n",
    "    clf = LinearSVC(max_iter=100)\n",
    "    pipeline = Pipeline([('vectorizer', vec), ('classifier', clf)])\n",
    "    pipeline.fit(train_sents, train_labels)\n",
    "    feature_name = 'tfidf'\n",
    "    if feature != 'tfidf':\n",
    "        feature_name = 'binary' if feature else 'count'\n",
    "    print('accuracy using {} for feature values'.format(feature_name))\n",
    "    print(accuracy_score(test_labels, pipeline.predict(test_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WcGoRRhu4-Bz"
   },
   "source": [
    "# ANN with average embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBvHaZMi4-B0"
   },
   "outputs": [],
   "source": [
    "def create_nn(embedding_matrix, num_classes):\n",
    "\n",
    "    i = Input((None,))\n",
    "  \n",
    "    embedding_layer = Embedding(\n",
    "      input_dim=len(du.vocab_dict)+2, \n",
    "      output_dim=du.vec_dim,\n",
    "      embeddings_initializer=Constant(embedding_matrix),\n",
    "      mask_zero=True,\n",
    "    )\n",
    "    reduce_sumer=Lambda(lambda x: tf.reduce_sum(x, 1) )\n",
    "    nz_counter = Lambda(lambda x: tf.to_float(tf.count_nonzero(x, 1, keepdims=True)))\n",
    "    division = Lambda(lambda x: tf.div(x[0], x[1]))\n",
    "    inp = embedding_layer(i)\n",
    "    inp = reduce_sumer(inp)\n",
    "    nz = nz_counter(i)\n",
    "    inp = division([inp, nz])\n",
    "  \n",
    "  \n",
    "    y = Dense(1024, activation='relu', input_shape=(du.vec_dim,))(inp)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    y = Dense(512, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    y = Dense(256, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "  \n",
    "    model = Model(inputs=i, outputs=y)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AicxC-4-4-B2"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "train_tok_sents = list(map(word_tokenize, train_sents))\n",
    "test_tok_sents = list(map(word_tokenize, test_sents))\n",
    "embedding_matrix = du.create_embedding_matrix()\n",
    "unk_id = du.vocab_dict['<unk>']\n",
    "x_train = pad_sequences([[du.vocab_dict.get(w, unk_id) for w in s] for s in train_tok_sents],\n",
    "                        padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "x_test = pad_sequences([[du.vocab_dict.get(w, unk_id) for w in s] for s in test_tok_sents],\n",
    "                       padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "y_train = to_categorical([du.label_dict[l] for l in train_labels])\n",
    "y_test = to_categorical([du.label_dict[l] for l in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "NNWWQnB74-B5",
    "outputId": "3bd21c6d-c326-4b12-dc6d-950db369cb46"
   },
   "outputs": [],
   "source": [
    "nn = create_nn(embedding_matrix, len(du.label_dict))\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "A7WoBf2N4-B7",
    "outputId": "55e9eddc-88c1-4477-ed68-b7d3fdc9e856",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn.fit(x_train, y_train,epochs=15, validation_data=(x_test, y_test) ,batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvH2JMSi4-B-"
   },
   "source": [
    "# RNN methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC7SkcZJn3z7"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfHUZ7H_4-B_"
   },
   "outputs": [],
   "source": [
    "class VariableLengthGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, X, Y, batch_size, num_classes, shuffle=False):\n",
    "\n",
    "    \n",
    "        self.x = np.array(X)\n",
    "        self.y = np.array(Y)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "  \n",
    "    def __len__(self):\n",
    "    \n",
    "        return int(np.ceil(len(self.x)/self.batch_size))\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x_batch = self.x[idx*self.batch_size: (idx+1)*self.batch_size]\n",
    "        y_batch = self.y[idx*self.batch_size: (idx+1)*self.batch_size]\n",
    "        if len(x_batch) != self.batch_size:\n",
    "            x_batch = self.x[len(self.x)-self.batch_size: len(self.x)]\n",
    "            y_batch = self.y[len(self.y)-self.batch_size: len(self.y)]\n",
    "\n",
    "        x = pad_sequences(x_batch, padding='post')\n",
    "        y = y_batch\n",
    "    \n",
    "        return (x, y)\n",
    "  \n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            shuffled_indexes = list(range(len(self.x)))\n",
    "            np.random.shuffle(shuffled_indexes)\n",
    "            self.x, self.y = self.x[shuffled_indexes], self.y[shuffled_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGab-eQJ4-CB"
   },
   "outputs": [],
   "source": [
    "def get_simple_bilstm(embedding_matrix, num_classes, num_layers, hidden_state_size):\n",
    "    \n",
    "    x = Input((None,))\n",
    "    embedding_layer = Embedding(\n",
    "      input_dim=len(du.vocab_dict)+2, \n",
    "      output_dim=du.vec_dim,\n",
    "      embeddings_initializer=Constant(embedding_matrix),\n",
    "      mask_zero=True,\n",
    "    )\n",
    "    \n",
    "    inp = embedding_layer(x)\n",
    "    \n",
    "    for i in range(num_layers-1):\n",
    "    \n",
    "        fw_rnn = LSTM(units=hidden_state_size, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        bw_rnn = LSTM(units=hidden_state_size, return_sequences=True, go_backwards=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        inp = concatenate([fw_rnn, bw_rnn], axis=-1) \n",
    "        \n",
    "    out_fw_rnn = LSTM(units=hidden_state_size, return_sequences=False)(inp)\n",
    "    out_bw_rnn = LSTM(units=hidden_state_size, return_sequences=False, go_backwards=True)(inp)\n",
    "    out_rnn = concatenate([out_fw_rnn, out_bw_rnn], axis=-1)\n",
    "    \n",
    "    y = Dense(256, activation='relu', input_shape=(hidden_state_size*2,))(out_rnn)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    y = Dense(128, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIS7-VXyfA9T"
   },
   "outputs": [],
   "source": [
    "def get_att_bilstm(embedding_matrix, num_classes, num_layers, hidden_state_size, att_layer_size=[]):\n",
    "    \n",
    "    x = Input((None,))\n",
    "    embedding_layer = Embedding(\n",
    "      input_dim=len(du.vocab_dict)+2, \n",
    "      output_dim=du.vec_dim,\n",
    "      embeddings_initializer=Constant(embedding_matrix),\n",
    "      mask_zero=True,\n",
    "    )\n",
    "    \n",
    "    inp = embedding_layer(x)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        fw_rnn = LSTM(units=hidden_state_size, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        bw_rnn = LSTM(units=hidden_state_size, return_sequences=True, go_backwards=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        inp = concatenate([fw_rnn, bw_rnn], axis=-1) \n",
    "    \n",
    "    alpha = inp\n",
    "    alpha = Lambda(lambda x: tf.math.tanh(x))(alpha)\n",
    "    for l in att_layer_size:\n",
    "      alpha = TimeDistributed(Dense(l, activation='relu'))(alpha)\n",
    "    alpha = TimeDistributed(Dense(1, activation='relu'))(alpha)\n",
    "    alpha = Lambda(lambda x: tf.reshape(x, [-1, tf.shape(x)[1]]))(alpha)\n",
    "    alpha = Lambda(lambda x: tf.nn.softmax(x))(alpha)\n",
    "    alpha = Lambda(lambda x: tf.reshape(x, [-1, tf.shape(x)[1], 1]))(alpha)\n",
    "    att_out = Lambda(lambda x: tf.math.multiply(x[0], x[1]))([alpha, inp])\n",
    "    att_out = Lambda(lambda x: tf.reduce_sum(x, axis=1))(att_out)\n",
    "    \n",
    "    \n",
    "    y = Dense(1024, activation='relu')(att_out)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(512, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(256, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNBGOizaDPkq"
   },
   "outputs": [],
   "source": [
    "def get_max_pool_bilstm(embedding_matrix, num_classes, num_layers, hidden_state_size):\n",
    "    \n",
    "    x = Input((None,))\n",
    "    embedding_layer = Embedding(\n",
    "      input_dim=len(du.vocab_dict)+2, \n",
    "      output_dim=du.vec_dim,\n",
    "      embeddings_initializer=Constant(embedding_matrix),\n",
    "      mask_zero=True,\n",
    "    )\n",
    "    \n",
    "    iden = Lambda(lambda x:x)\n",
    "    inp = iden(x)\n",
    "    inp = embedding_layer(inp)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        \n",
    "        fw_rnn = LSTM(units=hidden_state_size, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        bw_rnn = LSTM(units=hidden_state_size, return_sequences=True, go_backwards=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        inp = concatenate([fw_rnn, bw_rnn], axis=-1) \n",
    "    \n",
    "    inp = Lambda(lambda x: tf.reduce_max(x, axis=1))(inp)\n",
    "    \n",
    "    \n",
    "    y = Dense(1024, activation='relu')(inp)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(512, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(256, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpEW2tVY4-CE"
   },
   "outputs": [],
   "source": [
    "train_tok_sents = list(map(word_tokenize, train_sents))\n",
    "test_tok_sents = list(map(word_tokenize, test_sents))\n",
    "embedding_matrix = du.create_embedding_matrix()\n",
    "unk_id = du.vocab_dict['<unk>']\n",
    "x_train = [[du.vocab_dict.get(w, unk_id) for w in s] for s in train_tok_sents]\n",
    "x_test = [[du.vocab_dict.get(w, unk_id) for w in s] for s in test_tok_sents]\n",
    "y_train = to_categorical([du.label_dict[l] for l in train_labels])\n",
    "y_test = to_categorical([du.label_dict[l] for l in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1FqbgrY4-CG"
   },
   "outputs": [],
   "source": [
    "train_gen = VariableLengthGenerator(x_train, y_train, BATCH_SIZE, len(du.label_dict))\n",
    "valid_gen = VariableLengthGenerator(x_test, y_test, BATCH_SIZE, len(du.label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eqTkNMoq4-CJ",
    "outputId": "8635a8d6-d8c5-4421-d611-1889f586a9a6"
   },
   "outputs": [],
   "source": [
    "# nn = get_simple_bilstm(embedding_matrix, len(du.label_dict), num_layers=1, hidden_state_size=300)\n",
    "# nn = get_max_pool_bilstm(embedding_matrix, len(du.label_dict), num_layers=1, hidden_state_size=300)\n",
    "nn = get_att_bilstm(embedding_matrix, len(du.label_dict), num_layers=1, hidden_state_size=300)\n",
    "nn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "nn.summary()\n",
    "SVG(model_to_dot(nn).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "XCJ1RMsm4-CQ",
    "outputId": "69964f2a-b1f5-4ddf-8b5b-24c7a5b17d76"
   },
   "outputs": [],
   "source": [
    "hist = nn.fit_generator(\n",
    "    train_gen, \n",
    "    validation_data=valid_gen,\n",
    "    epochs=10,\n",
    "    \n",
    ")\n",
    "\n",
    "visualize_loss_and_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "uEXrSfyQbB9J",
    "outputId": "90d6dbb1-1e03-453b-9a3a-20f39ca85d67"
   },
   "outputs": [],
   "source": [
    "nn = get_simple_bilstm(embedding_matrix, len(du.label_dict), num_layers=1, hidden_state_size=300)\n",
    "# nn = get_max_pool_bilstm(embedding_matrix, len(du.label_dict), num_layers=1, hidden_state_size=300)\n",
    "# nn = get_att_bilstm(embedding_matrix, len(du.label_dict), num_layers=1, hidden_state_size=300)\n",
    "nn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "hist = nn.fit_generator(\n",
    "    train_gen, \n",
    "    validation_data=valid_gen,\n",
    "    epochs=6,\n",
    ")\n",
    "\n",
    "visualize_loss_and_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiZWJONaEAH-"
   },
   "source": [
    "# CNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT1eJPL1g1v2"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "train_tok_sents = list(map(word_tokenize, train_sents))\n",
    "test_tok_sents = list(map(word_tokenize, test_sents))\n",
    "embedding_matrix = du.create_embedding_matrix()\n",
    "unk_id = du.vocab_dict['<unk>']\n",
    "x_train = pad_sequences([[du.vocab_dict.get(w, unk_id) for w in s] for s in train_tok_sents],\n",
    "                        padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "x_test = pad_sequences([[du.vocab_dict.get(w, unk_id) for w in s] for s in test_tok_sents],\n",
    "                       padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "y_train = to_categorical([du.label_dict[l] for l in train_labels])\n",
    "y_test = to_categorical([du.label_dict[l] for l in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dkYZ7oOEC0b"
   },
   "outputs": [],
   "source": [
    "def get_cnn(embedding_matrix, num_classes, emb_method, kernel_sizes):\n",
    "  \n",
    "  x = Input((MAX_LEN,))\n",
    "  emb_init = None\n",
    "  emb_trainable = None\n",
    "  \n",
    "  if emb_method=='dynamic':\n",
    "    \n",
    "    emb_init = Constant(embedding_matrix)\n",
    "    emb_trainable = True\n",
    "  elif emb_method=='static':\n",
    "    \n",
    "    emb_init = Constant(embedding_matrix)\n",
    "    emb_trainable = False\n",
    "    \n",
    "  if emb_method=='rand':\n",
    "    \n",
    "    emb_init = 'uniform'\n",
    "    emb_trainable = True\n",
    "  \n",
    "  embedding_layer = Embedding(\n",
    "    input_dim=len(du.vocab_dict)+2, \n",
    "    output_dim=du.vec_dim,\n",
    "    embeddings_initializer=emb_init,\n",
    "#     mask_zero=True,\n",
    "    trainable = emb_trainable\n",
    "  )\n",
    "  inp = embedding_layer(x)\n",
    "  mask = Lambda( lambda x: K.cast(K.not_equal(x, 0), 'float32'))(x)\n",
    "  expanded_mask = Lambda(lambda x: K.tile(K.expand_dims(x, axis=2), n=[1, 1, du.vec_dim]))(mask)\n",
    "  inp = Multiply()([expanded_mask,  inp])\n",
    "\n",
    "  kernel_outs = []\n",
    "  for k in kernel_sizes:\n",
    "    \n",
    "    ker_out = Conv1D(2048, k, activation='relu')(inp)\n",
    "    ker_out = Dropout(0.4)(ker_out)\n",
    "    ker_out = GlobalAveragePooling1D()(ker_out)\n",
    "    kernel_outs.append(ker_out)\n",
    "  if len(kernel_outs)> 1:\n",
    "    kernel_concat = concatenate(kernel_outs, axis=-1)\n",
    "  else:\n",
    "    kernel_concat = kernel_outs[0]\n",
    "  y = Dense(1024, activation='relu')(kernel_concat)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = Dropout(0.6)(y)\n",
    "  y = Dense(512, activation='relu')(y)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = Dropout(0.6)(y)\n",
    "  y = Dense(128, activation='relu')(y)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = Dropout(0.6)(y)\n",
    "  y = Dense(num_classes, activation='softmax')(y)\n",
    "\n",
    "  model = Model(inputs=x, outputs=y)\n",
    "  return model\n",
    "    \n",
    "    \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6el6DKP0ajLI",
    "outputId": "a0b3b185-c930-49e9-a90f-410dc4bee139"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "nn = get_cnn(embedding_matrix, len(du.label_dict), 'dynamic', [3])\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "nn.summary()\n",
    "SVG(model_to_dot(nn).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "QkXlZ7zwbA1L",
    "outputId": "5fda8e7f-ede3-4f43-88de-3c3f913842bb"
   },
   "outputs": [],
   "source": [
    "hist = nn.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=256, epochs=10)\n",
    "visualize_loss_and_acc(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sW0lHK17fIq"
   },
   "source": [
    "# CNN+RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npxZAiGO_ZqQ"
   },
   "outputs": [],
   "source": [
    "def get_rnn_cnn(embedding_matrix, num_classes, num_layers, hidden_state_size):\n",
    "    \n",
    "    x = Input((None,))\n",
    "    embedding_layer = Embedding(\n",
    "      input_dim=len(du.vocab_dict)+2, \n",
    "      output_dim=du.vec_dim,\n",
    "      embeddings_initializer=Constant(embedding_matrix),\n",
    "      mask_zero=False,\n",
    "    )\n",
    "    \n",
    "    inp = embedding_layer(x)\n",
    "    mask = Lambda( lambda x: K.cast(K.not_equal(x, 0), 'float32'))(x)\n",
    "    expanded_mask = Lambda(lambda x: K.tile(K.expand_dims(x, axis=2), n=[1, 1, du.vec_dim]))(mask)\n",
    "    inp = Multiply()([expanded_mask,  inp])\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        fw_rnn = LSTM(units=hidden_state_size, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        bw_rnn = LSTM(units=hidden_state_size, return_sequences=True, go_backwards=True, dropout=0.2, recurrent_dropout=0.2)(inp)\n",
    "        inp = concatenate([fw_rnn, bw_rnn], axis=-1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    ker_out = Conv1D(512, 2, activation='relu')(inp)\n",
    "    ker_out = Dropout(0.3)(ker_out)\n",
    "    \n",
    "    to_dense = GlobalAveragePooling1D()(ker_out)\n",
    "    \n",
    "    \n",
    "    y = Dense(1024, activation='relu')(to_dense)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(512, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(256, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_aQz95rjl60x",
    "outputId": "0753dcc6-3735-4e7c-bbce-faea568079e7"
   },
   "outputs": [],
   "source": [
    "nn = get_rnn_cnn(embedding_matrix, len(du.label_dict), 1, 300)\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "nn.summary()\n",
    "SVG(model_to_dot(nn).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "G64UdpgOnVnG",
    "outputId": "bb89fcaa-5395-40ee-caf9-3cdeea9718fd"
   },
   "outputs": [],
   "source": [
    "hist = nn.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=256, epochs=10)\n",
    "visualize_loss_and_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9xLjOpgjEpX"
   },
   "outputs": [],
   "source": [
    "def get_cnn_rnn(embedding_matrix, num_classes, emb_method, kernel_sizes, att_layer_size=[]):\n",
    "  \n",
    "  x = Input((MAX_LEN,))\n",
    "  emb_init = None\n",
    "  emb_trainable = None\n",
    "  \n",
    "  if emb_method=='dynamic':\n",
    "    \n",
    "    emb_init = Constant(embedding_matrix)\n",
    "    emb_trainable = True\n",
    "  elif emb_method=='static':\n",
    "    \n",
    "    emb_init = Constant(embedding_matrix)\n",
    "    emb_trainable = False\n",
    "    \n",
    "  if emb_method=='rand':\n",
    "    \n",
    "    emb_init = 'uniform'\n",
    "    emb_trainable = True\n",
    "  \n",
    "  embedding_layer = Embedding(\n",
    "    input_dim=len(du.vocab_dict)+2, \n",
    "    output_dim=du.vec_dim,\n",
    "    embeddings_initializer=emb_init,\n",
    "#     mask_zero=True,\n",
    "    trainable = emb_trainable\n",
    "  )\n",
    "  inp = embedding_layer(x)\n",
    "  mask = Lambda( lambda x: K.cast(K.not_equal(x, 0), 'float32'))(x)\n",
    "  expanded_mask = Lambda(lambda x: K.tile(K.expand_dims(x, axis=2), n=[1, 1, du.vec_dim]))(mask)\n",
    "  inp = Multiply()([expanded_mask,  inp])\n",
    "\n",
    "  kernel_outs = []\n",
    "  for k in kernel_sizes:\n",
    "    \n",
    "    ker_out = Conv1D(256, k, activation='relu')(inp)\n",
    "    ker_out = Dropout(0.4)(ker_out)\n",
    "    ker_out = AveragePooling1D(5)(ker_out)\n",
    "    kernel_outs.append(ker_out)\n",
    "  if len(kernel_outs)> 1:\n",
    "    kernel_concat = concatenate(kernel_outs, axis=-1)\n",
    "  else:\n",
    "    kernel_concat = kernel_outs[0]\n",
    "    \n",
    "  fw_rnn = LSTM(units=256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(kernel_concat)\n",
    "  bw_rnn = LSTM(units=256, return_sequences=True, go_backwards=True, dropout=0.2, recurrent_dropout=0.2)(kernel_concat)\n",
    "  l_out = concatenate([fw_rnn, bw_rnn], axis=-1) \n",
    "    \n",
    "  alpha = l_out\n",
    "  for l in att_layer_size:\n",
    "    alpha = TimeDistributed(Dense(l, activation='relu'))(alpha)\n",
    "  alpha = TimeDistributed(Dense(1, activation='relu'))(alpha)\n",
    "  alpha = Lambda(lambda x: tf.reshape(x, [-1, tf.shape(x)[1]]))(alpha)\n",
    "  alpha = Lambda(lambda x: tf.nn.softmax(x))(alpha)\n",
    "  alpha = Lambda(lambda x: tf.reshape(x, [-1, tf.shape(x)[1], 1]))(alpha)\n",
    "  att_out = Lambda(lambda x: tf.math.multiply(x[0], x[1]))([alpha, l_out])\n",
    "  att_out = Lambda(lambda x: tf.reduce_sum(x, axis=1))(att_out)\n",
    "  y = Dense(1024, activation='relu')(att_out)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = Dropout(0.5)(y)\n",
    "  y = Dense(512, activation='relu')(y)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = Dropout(0.5)(y)\n",
    "  y = Dense(128, activation='relu')(y)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = Dropout(0.4)(y)\n",
    "  y = Dense(num_classes, activation='softmax')(y)\n",
    "\n",
    "  model = Model(inputs=x, outputs=y)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saOsLpzWjSu5"
   },
   "outputs": [],
   "source": [
    "nn = get_rnn_cnn(embedding_matrix, len(du.label_dict), 'dynamic', [2])\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "nn.summary()\n",
    "SVG(model_to_dot(nn).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CB22ANIijpXs"
   },
   "outputs": [],
   "source": [
    "hist = nn.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=256, epochs=10)\n",
    "visualize_loss_and_acc(hist)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
